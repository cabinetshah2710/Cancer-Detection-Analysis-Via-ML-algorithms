# -*- coding: utf-8 -*-
"""Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq4oAzwKusuNV0ep3kVX90Ran54x1neD
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("data.csv")

data.head()

print(" The shape of data is {}".format(data.shape))

print(data.columns)

import missingno as msno
msno.bar(data)

#dropping the unwanted column

data=data.drop(['id', 'Unnamed: 32'], axis = 1)

print(data.describe())

#performing Exploratory data analysis

sns.countplot(data['diagnosis'])

columns=['radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se', 'radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']

plt.figure(figsize=(18,38))
for i,cols in zip(range(1, 31),columns):
    plt.subplot(10, 3, i)
    plt.hist(data[cols])
    plt.title('Histogram distribution of {}'.format(cols))

print(data.corr().style.background_gradient(cmap='coolwarm', axis=None).set_precision(2))

# Define features and target varibles in the dataste
features = data.loc[:, data.columns != 'diagnosis']
target= data['diagnosis'] 
 

scaler = StandardScaler()
scaler.fit(features)
features_scaled = scaler.transform(features)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(features_scaled,target,test_size=0.3, random_state=42)



"""# K nearest neighbors"""

from sklearn.neighbors import KNeighborsClassifier
error_rate=[]
for i in range (1,31): 
    clf=KNeighborsClassifier(n_neighbors=i)
    clf.fit(X_train,y_train)
    predict_i=clf.predict(X_test)
    error_rate.append(np.mean(predict_i!=y_test))

plt.figure(figsize=(12,6))
plt.plot(range(1,31),error_rate,marker="o",markerfacecolor="green",
         linestyle="dashed",color="red",markersize=15)
plt.title("Error rate vs k value",fontsize=20)
plt.xlabel("k- values",fontsize=20)
plt.ylabel("error rate",fontsize=20)
plt.xticks(range(1,31))
plt.show()

clf=KNeighborsClassifier(n_neighbors=9).fit(X_train,y_train)
predict=clf.predict(X_test)

print(confusion_matrix(y_test,predict))

print(classification_report(y_test,predict))

"""# SVM"""

from sklearn.svm import SVC
classifier = SVC(C=2,kernel='rbf',cache_size=200)
classifier.fit(X_train,y_train)

pred= classifier.predict(X_test)

print(confusion_matrix(y_test,pred))

print(classification_report(y_test,pred))

"""# Logistic regression"""

from sklearn.linear_model import LogisticRegression
reg = LogisticRegression(solver='liblinear',C=1,class_weight="balanced").fit(X_train,y_train)

pred_lr= reg.predict(X_test)

print(confusion_matrix(y_test,pred_lr))

print(classification_report(y_test,pred_lr))

"""# Random forest"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=100,max_features='auto', max_depth=9).fit(X_train,y_train)
classifier_pred = classifier.predict(X_test)

print(confusion_matrix(y_test,classifier_pred))

print(classification_report(y_test,classifier_pred))

import joblib
filename = 'svm.pkl'
joblib.dump(classifier,filename)

