# -*- coding: utf-8 -*-
"""Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq4oAzwKusuNV0ep3kVX90Ran54x1neD
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("data.csv")

data.head()

print(" The shape of data is {}".format(data.shape))

print(data.columns)

#import missingno as msno
#msno.bar(data)

#dropping the unwanted column

data=data.drop(['id', 'Unnamed: 32'], axis = 1)

print(data.describe())

data.head()



#performing Exploratory data analysis

sns.countplot(data['diagnosis'])

columns=['radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se', 'radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']

plt.figure(figsize=(18,38))
for i,cols in zip(range(1, 31),columns):
    plt.subplot(10, 3, i)
    plt.hist(data[cols])
    plt.title('Histogram distribution of {}'.format(cols))

data.corr().style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)

for col_name in data.columns:
    if(data[col_name].dtype == 'object'):
        data[col_name]= data[col_name].astype('category')
        data[col_name] = data[col_name].cat.codes

# Define features and target varibles in the dataste
features = data.loc[:, data.columns != 'diagnosis']
target= data['diagnosis']

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt

model=ExtraTreesClassifier()
model.fit(features,target)

print(model.feature_importances_)

feature_importance=pd.Series(model.feature_importances_,index=features.columns)
feature_importance.nlargest(15).plot(kind='barh')
plt.show()

features.columns

X = data[['radius_mean','area_mean','concavity_worst','concavity_mean','concave points_worst','perimeter_worst','radius_worst', 'area_worst','concave points_mean',
          'area_se','texture_worst','compactness_worst','compactness_mean','smoothness_worst']]
y = data['diagnosis']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

x_train = scaler.fit_transform(X_train)
x_test = scaler.transform(X_test)



"""# K nearest neighbors"""

from sklearn.neighbors import KNeighborsClassifier
error_rate=[]
for i in range (1,31): 
    clf=KNeighborsClassifier(n_neighbors=i)
    clf.fit(x_train,y_train)
    predict_i=clf.predict(x_test)
    error_rate.append(np.mean(predict_i!=y_test))

plt.figure(figsize=(12,6))
plt.plot(range(1,31),error_rate,marker="o",markerfacecolor="green",
         linestyle="dashed",color="red",markersize=15)
plt.title("Error rate vs k value",fontsize=20)
plt.xlabel("k- values",fontsize=20)
plt.ylabel("error rate",fontsize=20)
plt.xticks(range(1,31))
plt.show()

clf=KNeighborsClassifier(n_neighbors=9).fit(x_train,y_train)
predict=clf.predict(X_test)

print(confusion_matrix(y_test,predict))

print(classification_report(y_test,predict))

"""# SVM"""

from sklearn.svm import SVC
classifier = SVC(C=2,kernel='rbf',cache_size=200)
classifier.fit(x_train,y_train)

pred= classifier.predict(x_test)

print(confusion_matrix(y_test,pred))

print(classification_report(y_test,pred))

"""# Logistic regression"""

from sklearn.linear_model import LogisticRegression
reg = LogisticRegression(solver='liblinear',C=1,class_weight="balanced").fit(x_train,y_train)

pred_lr= reg.predict(x_test)

print(confusion_matrix(y_test,pred_lr))

print(classification_report(y_test,pred_lr))

"""# Random forest"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=100,max_features='auto', max_depth=9).fit(x_train,y_train)
classifier_pred = classifier.predict(x_test)

print(confusion_matrix(y_test,classifier_pred))

print(classification_report(y_test,classifier_pred))

import pickle
pickle.dump(classifier, open('bcpmodel.pkl', 'wb'))
pickle.dump(scaler, open('features.pkl', 'wb'))

KNN = [0.97, 0.96, 0.96, 0.97]
SVM = [0.98, 0.97, 0.97, 0.98]
LR = [0.98, 0.98, 0.98, 0.98]


RF = [0.97, 0.96, 0.96, 0.96]

import matplotlib.pyplot as plt
import numpy as np


width = 0.1

r1 = np.arange(4)
r4 = [i + width for i in r1]
r5 = [i + width for i in r4]
r6 = [i + width for i in r5]

plt.bar(r1, KNN, color='#375e97', width=width, label='KNN')
plt.bar(r4, SVM, color='#3f681c', width=width, label='SVM')
plt.bar(r5, LR, color='#f18d9e', width=width, label='LR')
plt.bar(r6, RF, color='#4cb5f5', width=width, label='RF')

plt.ylim(top=1.2)  # adjust the top leaving bottom unchanged
plt.xlabel('Performance Measures', fontsize=16)
plt.ylabel('% Values', fontsize=16)
plt.tick_params(axis='both', which='major', labelsize=16)
plt.tick_params(axis='both', which='minor', labelsize=16)
plt.title('Train Results')

plt.xticks([r + 2.5*width for r in range(4)], [ 'Prec', 'Recal', 'F1','Acc'])

plt.legend(loc=2 , ncol=2, fontsize=12)
plt.show()

